# -*- coding: utf-8 -*-
"""KerasClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11uIDVW7DKBRoBVmqdK1FyjzE-1DtdNcH
"""

# import necessary modules
import keras
import numpy as np
from keras.models import Sequential # import the Sequential model from keras
from keras.layers import Dense, Dropout # import the Dense and Dropout layers from keras
from sklearn.preprocessing import MinMaxScaler # import the MinMaxScaler class from sklearn for preprocessing the dataset
from sklearn.model_selection import train_test_split #import the function to partition the dataset to train and test sets
from hyperopt import tpe, Trials, STATUS_OK # import the necessary functions for hypertuning from hyperopt
from hyperas import optim # import the optim function to perform the optimization for hyperparameter tuning
from hyperas.distributions import uniform, quniform, choice # import the hyperparameter tuning search space from hyperas

# create a data function to be parsed onto the optimization function of hypertuning
def data():
  dataset = load_wine() # load the dataset
  x = dataset.data # get the independent variables
  y = dataset.target # get the target classes column
  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) # splitting the dataset using the train_test_split where the test data contains 20% of the original dataset
  scale = MinMaxScaler() # create an object of MinMaxScaler class with default range of (0, 1)
  x_train = scale.fit_transform(x_train) # fit and transform the training independent variable to the MinMaxScaler object
  x_test = scale.transform(x_test) # transform the training dependent variable to the MinMaxScaler object
  return x_train, y_train, x_test, y_test

# create a model function to be parsed onto the optimization function of hypertuning
def model(x_train, y_train, x_test, y_test):
  model = Sequential() # create an object of the Sequential model of keras
  model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu')) # add the first layer with 13 neurons recieving a 13-dimensional input
  model.add(Dense(int({{quniform(1, 10, 1)}}), kernel_initializer='normal', activation='relu')) # add a second layer with tunable number of nodes found in the quantum uniform search space between 1 to 10
  model.add(Dense(3, kernel_initializer='normal', activation='softmax')) # make the classification layer with number of nodes equal to the number of classes 
  model.compile(loss='categorical_crossentropy', optimizer={{choice(['adam', 'sgd'])}}, metrics=['accuracy']) # compile the model with the loss function 'categorical_crossentropy' using either of the optimization method 'adam' or 'sgd' found by hyperparameter tuning and moreover, find the accuarcy of th model
  model.fit(x_train, y_train, verbose=10) # fit the model with the training set
  loss, acc = model.evaluate(x_test, y_test) # evaluate the model with the test dataset and find the loss and accuracy of the model.
  return {'loss': -acc, 'status': STATUS_OK, 'model': model} # return a dictionary containing the loss to be minimized, acc (here we take the negative of acc because we need to optimize it to the lowest possible value), an ok status info if executed well, and the model

'''
Setting up hyperparameter tuning using the hyperas package.
Take the  minimize function from the optim module and pass our model and data functions as strings.
The algorithm used for hyperparameter tuning is the tree-structured parzen estimator, max_evals determines how many times the minimization approach will run.
'trial' is where the results stored and here Trials() is taken from hyperopt and the name of the notebook is on which it is will get executed.
It will return the best run parameters and the best model.
'''

best_run, best_model = optim.minimize(model=model, # the model function 
                                      data=data, # the data function 
                                      algo=tpe.suggest, # the optimization method 
                                      max_evals=20, # the number of evaluation run
                                      trials=Trials(), # database to be stored
                                      notebook_name='KerasClassifier', 
                                      keep_temp=True)


loss, acc = best_model.evaluate(x_test, y_test) # evaluating the best model returned after hyperparameter tuning
print('Loss: ', loss)
print('Accuracy: ', acc)
print('best run: ',best_run)